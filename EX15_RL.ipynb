{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bonvech/MSU-AI/blob/main/EX15_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq_Q1hInNb7i"
      },
      "source": [
        "# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–¥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZk9Hw3HNb7l"
      },
      "source": [
        "–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∏–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiG9CAfjNb7m"
      },
      "outputs": [],
      "source": [
        "!pip install -q gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QtMgaMLNb7n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "from torch import nn\n",
        "from gym import Env, spaces\n",
        "from itertools import product\n",
        "from collections import deque\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57V-PaUwNb7o"
      },
      "source": [
        "–ß—Ç–æ–±—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–ª–∏—Å—å, –∑–∞—Ñ–∏–∫—Å–∏—Ä—É–µ–º seeds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c_7LirsNb7p"
      },
      "outputs": [],
      "source": [
        "def set_random_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "set_random_seed(42)\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcVZrxbBNb7p"
      },
      "source": [
        "# –ó–∞–¥–∞–Ω–∏–µ 1. –ù–∞–ø–∏—Å–∞–Ω–∏–µ —Å—Ä–µ–¥—ã –¥–ª—è –∏–≥—Ä—ã –≤ –∫—Ä–µ—Å—Ç–∏–∫–∏-–Ω–æ–ª–∏–∫–∏"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxIooQicNb7p"
      },
      "source": [
        "–í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –≤—ã –¥–æ–ª–∂–Ω—ã:\n",
        "\n",
        "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é —Å—Ä–µ–¥—É –¥–ª—è –∏–≥—Ä—ã –≤ –∫—Ä–µ—Å—Ç–∏–∫–∏ –Ω–æ–ª–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ `gymnasium.Env`, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤ –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞–Ω–∏—è—Ö.\n",
        "- –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É —Å—Ä–µ–¥—ã –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –∏–≥—Ä—ã –¥–≤—É—Ö —Å–ª—É—á–∞–π–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN2P0VvVNb7p"
      },
      "source": [
        "## –§–æ—Ä–º–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
        "* –°–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞—é—â–∞—è —Å—Ä–µ–¥–∞ `TicTacToeEnv`.\n",
        "* –ò–≥—Ä–∞—é—â–∏–µ —Å–ª—É—á–∞–π–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã.\n",
        "* –ü–æ—Å—á–∏—Ç–∞–Ω–Ω—ã–π –≤–∏–Ω—Ä–µ–π—Ç –¥–ª—è `'X'` –¥–ª—è –∏–≥—Ä—ã —Å–ª—É—á–∞–π–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤:\n",
        "\n",
        "    ```\n",
        "X wins in 58.53% games\n",
        "    ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySBgIJeiNb7q"
      },
      "source": [
        "## –°—Ä–µ–¥–∞ –¥–ª—è –∏–≥—Ä—ã –≤ –∫—Ä–µ—Å—Ç–∏–∫–∏-–Ω–æ–ª–∏–∫–∏\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faPvNTb8Nb7q"
      },
      "source": [
        "**–û–ø–∏—Å–∞–Ω–∏–µ —Å—Ä–µ–¥—ã:**\n",
        "\n",
        "1. –ò–≥—Ä–æ–≤–æ–µ –ø–æ–ª–µ –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä 3√ó3, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ —Ö–æ–¥—É –∏–≥—Ä—ã –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω—è—Ç—å—Å—è –º–∞—Ä–∫–µ—Ä–∞–º–∏ –∏–≥—Ä–æ–∫–æ–≤ `'X'` –∏ `'0'`. –í –∫–ª–∞—Å—Å–µ –Ω–∏–∂–µ –æ—Ç—Ä–∏—Å–æ–≤–∫–∞ –∏–≥—Ä–æ–≤–æ–≥–æ –ø–æ–ª—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –≤ –º–µ—Ç–æ–¥–µ `render`.\n",
        "\n",
        "    –ü—Ä–∏–º–µ—Ä –∏–≥—Ä–æ–≤–æ–≥–æ –ø–æ–ª—è:\n",
        "\n",
        "    ```\n",
        "0|X|0\n",
        "_|X|0\n",
        "X|_|_\n",
        "    ```\n",
        "2. –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –≤–µ–∫—Ç–æ—Ä–æ–º (—Å–ø–∏—Å–∫–æ–º) –∏–∑ 9 —á–∏—Å–µ–ª, –≤ –∫–æ—Ç–æ—Ä–æ–º 0 –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç –Ω–µ–∑–∞–Ω—è—Ç—É—é —è—á–µ–π–∫—É, 1 ‚Äî —è—á–µ–π–∫—É, –∑–∞–Ω—è—Ç—É—é `'X'`, –∏ ‚àí1 ‚Äî —è—á–µ–π–∫—É, –∑–∞–Ω—è—Ç—É—é `'0'`. –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ –∞—Ç—Ä–∏–±—É—Ç–µ –∫–ª–∞—Å—Å–∞ `self.cells`.\n",
        "\n",
        "    –ü—Ä–∏–º–µ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å—Ä–µ–¥—ã –¥–ª—è –∏–≥—Ä–æ–≤–æ–≥–æ –ø–æ–ª—è –≤—ã—à–µ:\n",
        "    ```\n",
        "[-1, 1, -1, 0, 1, -1, 1, 0, 0]\n",
        "    ```\n",
        "3. –í –∞—Ç—Ä–∏–±—É—Ç–µ `self.player` —Ö—Ä–∞–Ω–∏—Ç—Å—è `X` –∏–ª–∏ `0` ‚Äî —Å–∏–º–≤–æ–ª –∏–≥—Ä–æ–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–µ–π—á–∞—Å —Ö–æ–¥–∏—Ç (–º–µ–Ω—è–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ä–µ–¥—ã).\n",
        "\n",
        "4. –í –º–µ—Ç–æ–¥ `step` –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è `action` ‚Äî –Ω–æ–º–µ—Ä —è—á–µ–π–∫–∏, –∫–æ—Ç–æ—Ä—É—é –∏–≥—Ä–æ–∫ —Ö–æ—á–µ—Ç –∏–∑–º–µ–Ω–∏—Ç—å. –ê–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –ø–æ—Å—Ç–∞–≤–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –º–∞—Ä–∫–µ—Ä —Ç–æ–ª—å–∫–æ –≤ –Ω–µ–∑–∞–Ω—è—Ç—É—é —è—á–µ–π–∫—É –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º –ø–µ—Ä–µ–¥–∞—á–∏ –Ω–æ–º–µ—Ä–∞ —è—á–µ–π–∫–∏ –≤ —Å—Ä–µ–¥—É.\n",
        "\n",
        "5. –ò–≥—Ä–∞ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è (`self.done = True`) –≤ –¥–≤—É—Ö —Å–ª—É—á–∞—è—Ö: –ø–æ–±–µ–¥–∞ –æ–¥–Ω–æ–≥–æ –∏–∑ –∏–≥—Ä–æ–∫–æ–≤ (–ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –º–µ—Ç–æ–¥–æ–º `self.check_for_win`) –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—É—Å—Ç—ã—Ö –∫–ª–µ—Ç–æ–∫ –Ω–∞ –ø–æ–ª–µ (–ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –º–µ—Ç–æ–¥–æ–º `self.check_for_draw`).\n",
        "\n",
        "6. –ù–∞–≥—Ä–∞–¥–∞ –¥–æ–ª–∂–Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å—Å—è –∑–∞ –ø–æ–±–µ–¥—É `'X'` –≤ —Ä–∞–∑–º–µ—Ä–µ +1 –æ—á–∫–∞, –∑–∞ –ø–æ–±–µ–¥—É `'0'`, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, ‚àí1. –í —Å–ª—É—á–∞–µ –Ω–∏—á—å–µ–π –∏ –Ω–µ—Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –Ω–∞–≥—Ä–∞–¥–∞ —Ä–∞–≤–Ω–∞ 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP3gL-x-Nb7r"
      },
      "source": [
        "–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–æ–¥–∞ `# Your code here`. –ü–æ–º–Ω–∏—Ç–µ, —á—Ç–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ `self.action_space`, –∑–∞–ø–æ–ª–Ω—è–µ–º–æ–µ –∏–≥—Ä–æ–∫–∞–º–∏, [–¥–∏—Å–∫—Ä–µ—Ç–Ω–æ üõ†Ô∏è[doc]](https://gymnasium.farama.org/api/spaces/fundamental/#discrete)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKGQEJliNb7r"
      },
      "outputs": [],
      "source": [
        "class TicTacToeEnv(Env):\n",
        "    def __init__(self):\n",
        "        # Define default variable\n",
        "        self.cells = [0 for i in range(9)]  # environment state\n",
        "        self.player = \"X\"  # current player (changes every step)\n",
        "        self.done = False  # is the game over\n",
        "        self.winner = None  # who is the winner\n",
        "\n",
        "        # Symbols for rendering\n",
        "        self.markers = {1: \"X\", 0: \"_\", -1: \"0\"}\n",
        "\n",
        "        # Space https://gymnasium.farama.org/api/spaces/fundamental\n",
        "        self.action_space = spaces.Discrete(9)  # Your code here\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Bring game to initial state, define default variables.\n",
        "        \"\"\"\n",
        "        # Your code here\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        –†rint game board.\n",
        "        \"\"\"\n",
        "        cells = [self.markers[x] for x in self.cells]\n",
        "\n",
        "        for j in range(0, 9, 3):\n",
        "            print(\"|\".join([cells[i] for i in range(j, j + 3)]))\n",
        "\n",
        "    def legal_actions(self):\n",
        "        \"\"\"\n",
        "        Check for actions available: check free cells\n",
        "        \"\"\"\n",
        "        return [ind for ind, value in enumerate(self.cells) if value == 0]\n",
        "\n",
        "    def check_for_win(self, cells):\n",
        "        \"\"\"\n",
        "        Check that there is any win combination on the board.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cells: list\n",
        "            Environment state\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            bool\n",
        "            True if win, False in over cases\n",
        "        \"\"\"\n",
        "        # Your code here\n",
        "\n",
        "    def check_for_draw(self, cells):\n",
        "        \"\"\"\n",
        "        Checking that the board is completely filled out.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cells: list\n",
        "            Environment state\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            bool\n",
        "            True if the board is completely filled out, False in over cases\n",
        "        \"\"\"\n",
        "        if 0 not in cells:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Player input process\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        action: int\n",
        "            number of cell for change\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        observation: list\n",
        "            New environment state\n",
        "        reward: int\n",
        "            Reward: 1 if win of 'X', -1 if win of '0', 0 in othrer cases\n",
        "        self.done: bool\n",
        "            Game over flag\n",
        "        self.player: 'X' or '0'\n",
        "            Player who takes the next step\n",
        "        \"\"\"\n",
        "        # Check that action is possible\n",
        "        assert self.action_space.contains(action), \"impossible action\"\n",
        "        # Check that cell is empty\n",
        "        assert (\n",
        "            action in self.legal_actions()\n",
        "        ), \"not legal action\"\n",
        "\n",
        "        # Fill self.cells[action] depends on on whose turn (self.player) it is\n",
        "        self.cells[action] =  # Your code here\n",
        "\n",
        "        observation = # Your code here\n",
        "\n",
        "        # Check that there is any win combination on the board\n",
        "        self.done =   # Your code here\n",
        "\n",
        "        if self.done and self.player == \"X\":\n",
        "            reward = 1\n",
        "            self.winner = \"X\"\n",
        "\n",
        "        elif self.done and self.player == \"0\":\n",
        "            reward = -1\n",
        "            self.winner = \"0\"\n",
        "\n",
        "        else:\n",
        "            # Checking that the board is completely filled out\n",
        "            self.done = self.check_for_draw(self.cells)\n",
        "            reward = 0\n",
        "            self.winner = None\n",
        "\n",
        "        # Toggle players\n",
        "        if self.player == \"X\":\n",
        "            self.player = \"0\"\n",
        "        else:\n",
        "            self.player = \"X\"\n",
        "\n",
        "        return observation, reward, self.done, self.player"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLXSJn3RNb7r"
      },
      "source": [
        "## –°–ª—É—á–∞–π–Ω—ã–π –∞–≥–µ–Ω—Ç"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNhOrHUGNb7s"
      },
      "source": [
        "–†–µ–∞–ª–∏–∑—É–π—Ç–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_GQBzpPNb7s"
      },
      "outputs": [],
      "source": [
        "class RandomAgent:\n",
        "    def __init__(self, mark=\"X\"):\n",
        "        self.mark = mark\n",
        "\n",
        "    def get_action(self, env):\n",
        "        \"\"\"\n",
        "        Sample random LEGAL action from action space\n",
        "        (use env.legal_actions and random.choice)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        action: int\n",
        "            number of cell for change\n",
        "        \"\"\"\n",
        "        # Your code here\n",
        "\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXQOVPjsNb7s"
      },
      "outputs": [],
      "source": [
        "ttt = TicTacToeEnv()\n",
        "\n",
        "x_agent = RandomAgent(\"X\")\n",
        "o_agent = RandomAgent(\"0\")\n",
        "\n",
        "rand_players = {\"X\": x_agent, \"0\": o_agent}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9qyQ5nQNb7s"
      },
      "source": [
        "–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∏–≥—Ä—É –º–µ–∂–¥—É –¥–≤—É–º—è —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrj8-ZCSNb7t"
      },
      "outputs": [],
      "source": [
        "ttt.reset()\n",
        "\n",
        "while not ttt.done:\n",
        "    # which agent from `rand_players` is playing (use `ttt.player` info)\n",
        "    player = # Your code here\n",
        "    # action of this agent\n",
        "    action =  # Your code here\n",
        "    # step\n",
        "    state, reward, done, player =  # Your code here\n",
        "    ttt.render()\n",
        "    print(\"\\n\")\n",
        "print(f\"{ttt.winner} wins! Reward is {reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu-wI72cNb7t"
      },
      "source": [
        "## –í–∏–Ω—Ä–µ–π—Ç –¥–ª—è 'X' –¥–ª—è —Å–ª—É—á–∞–π–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_PEZMeINb7t"
      },
      "source": [
        "–î–∞–≤–∞–π—Ç–µ —Å–¥–µ–ª–∞–µ–º –±–µ–π–∑–ª–∞–π–Ω, —Å –∫–æ—Ç–æ—Ä—ã–º –º—ã –±—É–¥–µ–º —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–≥—Ä—ã –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –±—É–¥–µ–º –æ–±—É—á–∞—Ç—å. –î–ª—è —ç—Ç–æ–≥–æ –ø–æ—Å—á–∏—Ç–∞–µ–º, –≤ –∫–∞–∫–æ–º –ø—Ä–æ—Ü–µ–Ω—Ç–µ —Å–ª—É—á–∞–µ–≤ `X` –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç –Ω–∞ 100000 –∏–≥—Ä–∞—Ö –º–µ–∂–¥—É —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∏–≥—Ä–æ–∫–∞–º–∏, –∏ –¥–∞–ª—å—à–µ –±—É–¥–µ–º –ø—Ä–æ–±–æ–≤–∞—Ç—å —É–ª—É—á—à–∏—Ç—å —ç—Ç–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjdiZCeXNb7t"
      },
      "outputs": [],
      "source": [
        "wins = {\"X\": 0, \"0\": 0}\n",
        "\n",
        "for i in range(100_000):\n",
        "    ttt.reset()\n",
        "\n",
        "    while not ttt.done:\n",
        "        player = rand_players[ttt.player]\n",
        "        action = player.get_action(ttt)\n",
        "\n",
        "        state, reward, done, player = ttt.step(action)\n",
        "\n",
        "    if ttt.winner is not None:\n",
        "        wins[ttt.winner] += 1\n",
        "\n",
        "print(f'X wins in {round((wins[\"X\"]/100_000)*100, 2)}% games')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0_zhutaNb7u"
      },
      "source": [
        "# –ó–∞–¥–∞–Ω–∏–µ 2. –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –∏–≥—Ä–µ –≤ –∫—Ä–µ—Å—Ç–∏–∫–∏-–Ω–æ–ª–∏–∫–∏ —Å –ø–æ–º–æ—â—å—é Q-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZPZv0-6Nb7u"
      },
      "source": [
        "–°–æ–∑–¥–∞–π—Ç–µ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∏–≥—Ä—ã –≤ –∫—Ä–µ—Å—Ç–∏–∫–∏-–Ω–æ–ª–∏–∫–∏ –∏ –æ–±—É—á–∏—Ç–µ –µ–≥–æ —Å –ø–æ–º–æ—â—å—é Q-learning, –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –∂–∞–¥–Ω–æ–≥–æ –∏ $\\varepsilon$-–∂–∞–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIj6iqUQNb7u"
      },
      "source": [
        "## –§–æ—Ä–º–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
        "\n",
        "1. –û–±—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é Q-learning –∞–≥–µ–Ω—Ç—ã, –∏–≥–∞—é—â–∏–µ –∑–∞ `'X'`:\n",
        "- –∂–∞–¥–Ω—ã–π,\n",
        "- $\\varepsilon$-–∂–∞–¥–Ω—ã–π.\n",
        "2. –ü–æ—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –≤–∏–Ω—Ä–µ–π—Ç—ã –ø—Ä–∏ –∏–≥—Ä–µ –ø—Ä–æ—Ç–∏–≤ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, –∏–≥—Ä–∞—é—â–µ–≥–æ –∑–∞ `'0'`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HadW239QNb7v"
      },
      "source": [
        "## –ö–æ–¥ Q-learning –∞–≥–µ–Ω—Ç–∞\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z443qhf9Nb7v"
      },
      "source": [
        "–í —ç—Ç–æ–π —á–∞—Å—Ç–∏ –∑–∞–¥–∞–Ω–∏—è –≤–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å Q-learning –∞–≥–µ–Ω—Ç–∞.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu_pKmfPNb7v"
      },
      "source": [
        "–í–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–∏—Ç—å `# Your code here`:\n",
        "- –º–µ—Ç–æ–¥ `set_states`: –Ω—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π ‚àí1, 0 –∏ 1 –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å—Ä–µ–¥—ã –ø–µ—Ä–µ–¥ —Ö–æ–¥–æ–º `‚ÄòX‚Äô` (`‚ÄòX‚Äô` –≤—Å–µ–≥–¥–∞ —Ö–æ–¥–∏—Ç –ø–µ—Ä–≤—ã–º, –ø–æ—ç—Ç–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ 1 –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å—Ä–µ–¥—ã –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–∞–≤–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É ‚àí1) –∏ –ø–µ—Ä–µ–¥ —Ö–æ–¥–æ–º `‚Äô0‚Äô` (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ 1 –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å—Ä–µ–¥—ã –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–∞ –æ–¥–∏–Ω –±–æ–ª—å—à–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ ‚àí1).\n",
        "- –º–µ—Ç–æ–¥ `set_Q_table`: –Ω—É–∂–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ª—É—á–∞–π–Ω—ã–º–∏ —á–∏—Å–ª–∞–º–∏ Q-–∑–Ω–∞—á–µ–Ω–∏—è –≤—Å–µ—Ö –ª–µ–≥–∞–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π (–ª–µ–≥–∞–ª—å–Ω–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö –∫–ª–µ—Ç–æ–∫) –∏–∑ —ç—Ç–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π.\n",
        "- –º–µ—Ç–æ–¥ `get_action`: –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –∏–ª–∏ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∞–≥–µ–Ω—Ç–∞ –∏ –∑–Ω–∞—á–µ–Ω–∏—è `epsilon`.\n",
        "- –º–µ—Ç–æ–¥ `update_Q`: –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é —Ñ–æ—Ä–º—É–ª—É Q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b8xv8adNb7v"
      },
      "source": [
        "–û—Å–Ω–æ–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ Q-learning:\n",
        "\n",
        "$$Q(s,a) = Q(s,a)+Œ±(R^a_{s} + \\gamma\\max_{a'}Q(s',a') -Q(s,a)),$$\n",
        "\n",
        "–≥–¥–µ $s$ ‚Äî —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã `state` –≤ –Ω–∞—á–∞–ª–µ —Ö–æ–¥–∞ –∞–≥–µ–Ω—Ç–∞, $a$ ‚Äî –¥–µ–π—Å—Ç–≤–∏–µ `action` –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –¥–∞–Ω–Ω–æ–º —Ö–æ–¥–µ, $Q(s,a)$ ‚Äî –∑–Ω–∞—á–µ–Ω–∏–µ Q-table `self.Q[current_state][action]` –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è $s$ –∏ –¥–µ–π—Å—Ç–≤–∏—è $a$, $R^a_{s}$ ‚Äî –Ω–∞–≥—Ä–∞–¥–∞ `reward` –∑–∞ –¥–µ–π—Å—Ç–≤–∏–µ `a` –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è `s`, $s'$ ‚Äî —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã –ø–æ—Å–ª–µ —Ö–æ–¥–∞ –∏–≥—Ä–æ–∫–∞ –∏ –µ–≥–æ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞, $a'$ ‚Äî —Å–ª–µ–¥—É—é—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –∏–≥—Ä–æ–∫–∞, $Œ±$ ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è, $\\gamma$ ‚Äî –¥–∏—Å–∫–æ–Ω—Ç –∑–∞ –¥–ª–∏–Ω–Ω—É—é –∏–≥—Ä—É."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF8VEXunNb7w"
      },
      "source": [
        "**–°–æ–≤–µ—Ç:**\n",
        "- –ü—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ Q-–∑–Ω–∞—á–µ–Ω–∏–π (–º–µ—Ç–æ–¥ `update_Q`) —É—á—Ç–∏—Ç–µ, —á—Ç–æ —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–≥—Ä—ã –Ω–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ Q-—Ç–∞–±–ª–∏—Ü–µ (–∏–∑ –Ω–µ–≥–æ —É–∂–µ –Ω–µ–ª—å–∑—è –¥–µ–ª–∞—Ç—å —Ö–æ–¥) –∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏–π, –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–∏—Ö –µ–º—É, $\\max_{a'}Q(s',a')$ –±—É–¥–µ—Ç —Ä–∞–≤–Ω–æ –Ω—É–ª—é.\n",
        "- –ü—Ä–∏ –ø–æ–±–µ–¥–µ `'X'` –≤—ã–¥–∞–µ—Ç—Å—è –Ω–∞–≥—Ä–∞–¥–∞ +1, –∞ –≤ —Å–ª—É—á–∞–µ –ø–æ–±–µ–¥—ã `'0'` ‚Äî –Ω–∞–≥—Ä–∞–¥–∞ ‚àí1. –ê–≥–µ–Ω—Ç, –∏–≥—Ä–∞—é—â–∏–π `'X',` –¥–æ–ª–∂–µ–Ω –≤—ã–±–∏—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º Q-–∑–Ω–∞—á–µ–Ω–∏–µ–º, –∞ `'0'` ‚Äî —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º Q-–∑–Ω–∞—á–µ–Ω–∏–µ–º.\n",
        "- –ü—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ Q-–∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –¥–µ–π—Å—Ç–≤–∏–π –∏–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —É—á—Ç–∏—Ç–µ, —á—Ç–æ $s'$ –±—É–¥–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –∏–≥—Ä–æ–≤–æ–≥–æ –ø–æ–ª—è –Ω–µ –ø–æ—Å–ª–µ —Ö–æ–¥–∞ –∏–≥—Ä–æ–∫–∞, –∞ –ø–æ—Å–ª–µ —Ö–æ–¥–∞ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URTB0BEoNb7w"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "\n",
        "class QTableAgent:\n",
        "    def __init__(\n",
        "            self, alpha=0.05, gamma=0.9, mark=\"X\", epsilon=0.,\n",
        "            epsilon_off=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        alpha: float\n",
        "            learning rate\n",
        "        gamma: float\n",
        "            discount coefficient\n",
        "        mark: str\n",
        "            'X' or '0' - player symbol\n",
        "        epsilon: float\n",
        "            epsilon for epsilon-greedy agent\n",
        "        epsilon_off: boolean\n",
        "            if True -'greedy' learning strategy or inference,\n",
        "            if False 'epsilon-greedy' learning strategy\n",
        "        \"\"\"\n",
        "        self.mark = mark\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_off = epsilon_off\n",
        "\n",
        "        # Get possible for self.mark environment states\n",
        "        self.states = self.set_states()\n",
        "        # Init Q-table\n",
        "        self.Q = self.set_Q_table()\n",
        "\n",
        "    def set_states(self):\n",
        "        \"\"\"\n",
        "        Set possible for self.mark environment states\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        states: set\n",
        "            Set of possible for self.mark environment states\n",
        "        \"\"\"\n",
        "        # Set of all possible marker compositions\n",
        "        states = set(product(*[list(range(-1, 2)) for _ in range(9)]))\n",
        "        # Subset of states for X player\n",
        "        # contains states in which both players took equal number of actions\n",
        "        # (since X goes first)\n",
        "        if self.mark == \"X\": # select with condition\n",
        "            states = { # Your code here\n",
        "\n",
        "        # Subset of states for 0 player\n",
        "        # contains states in which X player took one more action than 0 player\n",
        "        # (since 0 goes second)\n",
        "\n",
        "        elif self.mark == \"0\": # select with condition\n",
        "            states = { # Your code here\n",
        "        return states\n",
        "\n",
        "    def set_Q_table(self):\n",
        "        \"\"\"\n",
        "        Init Q-table.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Q: dict\n",
        "            Q-table[state][action] for possible states and actions with\n",
        "            random gauss (mean=0, sigma=0.1)\n",
        "        \"\"\"\n",
        "        Q = {}\n",
        "        # Match legal actions for each possible action in each state with\n",
        "        # random initial number\n",
        "        for state in self.states:\n",
        "            Q[state] = {}\n",
        "            # Your code here\n",
        "        return Q\n",
        "\n",
        "    def get_action(self, env):\n",
        "        \"\"\"\n",
        "        Sample optimal or random action.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        env: TicTacToeEnv\n",
        "            environment\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        action: int\n",
        "            number of cell for change\n",
        "        \"\"\"\n",
        "        state = tuple(env.cells)\n",
        "        rand = random.uniform(0, 1)\n",
        "\n",
        "        if self.epsilon_off or rand >= self.epsilon:\n",
        "            # Sample optimal action (based on greediness)\n",
        "            if self.mark == \"X\":\n",
        "                action = # Your code here\n",
        "            else:\n",
        "                action = # Your code here\n",
        "        else:\n",
        "            # Sample random  action\n",
        "            action = # Your code here\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update_Q(self, current_state, action, next_state, reward, done):\n",
        "        \"\"\"\n",
        "        Q-table update.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        current_state: list\n",
        "            Current environment state\n",
        "        action: int\n",
        "            Current agent action\n",
        "        next_state: list\n",
        "            Environment state after agent action and opponent action\n",
        "        reward: int\n",
        "            Reward\n",
        "        done: bool\n",
        "            Game over flag\n",
        "        \"\"\"\n",
        "        current_state = tuple(current_state)\n",
        "        if not done:\n",
        "            next_state = tuple(next_state)\n",
        "            next_state_value = max(self.Q[next_state], key=self.Q[next_state].get)\n",
        "        else:\n",
        "            next_state_value = 0\n",
        "\n",
        "        # Q-learning update folmula\n",
        "        self.Q[current_state][action] = # Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOqz94hbNb7x"
      },
      "source": [
        "## –û–±—É—á–µ–Ω–∏–µ –∂–∞–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ `'X'`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLScGLOYNb7x"
      },
      "source": [
        "–û–±—É—á–∏—Ç–µ –∞–≥–µ–Ω—Ç–∞, –∏–≥—Ä–∞—é—â–µ–≥–æ –∑–∞ `X`, –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞—é—â–µ–≥–æ—Å—è –∂–∞–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, –ø—Ä–æ—Ç–∏–≤ **—Å–ª—É—á–∞–π–Ω–æ–≥–æ** –∞–≥–µ–Ω—Ç–∞, –∏–≥—Ä–∞—é—â–µ–≥–æ –∑–∞ `0`, –Ω–∞ 1 –º–∏–ª–ª–∏–æ–Ω–µ –∏–≥—Ä –∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ –∏—Ö –≤–∏–Ω—Ä–µ–π—Ç—ã –º–µ–∂–¥—É —Å–æ–±–æ–π –∏ —Å –±–µ–π–∑–ª–∞–π–Ω–æ–º."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwFDl1a7Nb7x"
      },
      "source": [
        "**–°–æ–≤–µ—Ç:**\n",
        "- –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —É—á—Ç–∏—Ç–µ, —á—Ç–æ $s'$ –±—É–¥–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –∏–≥—Ä–æ–≤–æ–≥–æ –ø–æ–ª—è –Ω–µ –ø–æ—Å–ª–µ —Ö–æ–¥–∞ –∏–≥—Ä–æ–∫–∞, –∞ –ø–æ—Å–ª–µ —Ö–æ–¥–∞ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞.\n",
        "- –ü–æ–º–Ω–∏—Ç–µ, —á—Ç–æ `list` –≤ python —è–≤–ª—è–µ—Ç—Å—è [–∏–∑–º–µ–Ω—è–µ–º—ã–º —Ç–∏–ø–æ–º –¥–∞–Ω–Ω—ã—Ö ‚úèÔ∏è[blog]](https://realpython.com/python-mutable-vs-immutable-types/). –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `.copy()`, —á—Ç–æ–±—ã –∏–∑–æ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã, –ø–æ–¥–∞–≤–∞–µ–º–æ–µ –∞–≥–µ–Ω—Ç—É."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQMQGcuVNb7y"
      },
      "outputs": [],
      "source": [
        "ttt = TicTacToeEnv()\n",
        "\n",
        "x_agent = QTableAgent(0.05, 0.9, \"X\", epsilon=0.1, epsilon_off=True)\n",
        "\n",
        "o_agent = RandomAgent(\"0\")\n",
        "\n",
        "players = {\"X\": x_agent, \"0\": o_agent}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OsfZxllNb7y"
      },
      "outputs": [],
      "source": [
        "for i in range(1_000_000):\n",
        "    ttt.reset()\n",
        "    while not ttt.done:\n",
        "        player = players[ttt.player]\n",
        "        if player.mark == \"X\":\n",
        "            action_x =   # Your code here\n",
        "            current_state =  # Your code here\n",
        "            state, reward, done, _ =  # Your code here\n",
        "            if done:\n",
        "\n",
        "                players[\"X\"].update_Q( # Your code here\n",
        "        else:\n",
        "            action_o = # Your code here\n",
        "            next_state, reward, done, _ = # Your code here\n",
        "\n",
        "            players[\"X\"].update_Q( # Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zKIo4ADNb7y"
      },
      "source": [
        "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –≤–∏–Ω—Ä–µ–π—Ç:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ufm7r908Nb7y"
      },
      "outputs": [],
      "source": [
        "wins = {\"X\": 0, \"0\": 0}\n",
        "\n",
        "for i in range(100_000):\n",
        "    ttt.reset()\n",
        "\n",
        "    while not ttt.done:\n",
        "        player = players[ttt.player]\n",
        "        action = player.get_action(ttt)\n",
        "\n",
        "        state, reward, done, player = ttt.step(action)\n",
        "\n",
        "    if ttt.winner is not None:\n",
        "        wins[ttt.winner] += 1\n",
        "\n",
        "print(f'X wins in {round((wins[\"X\"]/100000)*100, 2)}% games')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66XIW2OHNb7y"
      },
      "source": [
        "## –û–±—É—á–µ–Ω–∏–µ $\\varepsilon$-–∂–∞–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ `'X'`:\n",
        "\n",
        "–û–±—É—á–∏—Ç–µ –∞–≥–µ–Ω—Ç–∞, –∏–≥—Ä–∞—é—â–µ–≥–æ –∑–∞ `X`, –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞—é—â–µ–≥–æ—Å—è $\\varepsilon$-–∂–∞–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, –ø—Ä–æ—Ç–∏–≤ **—Å–ª—É—á–∞–π–Ω–æ–≥–æ** –∞–≥–µ–Ω—Ç–∞, –∏–≥—Ä–∞—é—â–µ–≥–æ –∑–∞ `0`, –Ω–∞ 1 –º–∏–ª–ª–∏–æ–Ω–µ –∏–≥—Ä –∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ –∏—Ö –≤–∏–Ω—Ä–µ–π—Ç—ã –º–µ–∂–¥—É —Å–æ–±–æ–π –∏ —Å –±–µ–π–∑–ª–∞–π–Ω–æ–º."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0thDnKWNb74"
      },
      "outputs": [],
      "source": [
        "ttt = TicTacToeEnv()\n",
        "\n",
        "x_agent = QTableAgent(0.05, 0.9, \"X\", epsilon=0.1, epsilon_off=False)\n",
        "o_agent = RandomAgent(\"0\")\n",
        "\n",
        "players = {\"X\": x_agent, \"0\": o_agent}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv7bEbbXNb74"
      },
      "outputs": [],
      "source": [
        "for i in range(1_000_000):\n",
        "    ttt.reset()\n",
        "    while not ttt.done:\n",
        "        player = players[ttt.player]\n",
        "        if player.mark == \"X\":\n",
        "            action_x = # Your code here\n",
        "            current_state = # Your code here\n",
        "            state, reward, done, _ = # Your code here\n",
        "            if done:\n",
        "                players[\"X\"].update_Q( # Your code here\n",
        "        else:\n",
        "            action_o = # Your code here\n",
        "            next_state, reward, done, _ = # Your code here\n",
        "\n",
        "            players[\"X\"].update_Q( # Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKUYtmkpNb74"
      },
      "source": [
        "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –≤–∏–Ω—Ä–µ–π—Ç. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ç–æ, —á—Ç–æ –º—ã **–≤—ã–∫–ª—é—á–∞–µ–º –ø—Ä–∏–º–µ—à–∏–≤–∞–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏**. –ë–µ–∑ —ç—Ç–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –±—É–¥–µ—Ç —Å–∏–ª—å–Ω–æ –∑–∞–Ω–∏–∂–µ–Ω–æ!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FfEjvHxNb75"
      },
      "outputs": [],
      "source": [
        "wins = {\"X\": 0, \"0\": 0}\n",
        "players[\"X\"].epsilon_off = True\n",
        "\n",
        "for i in range(100_000):\n",
        "    ttt.reset()\n",
        "\n",
        "    while not ttt.done:\n",
        "        player = players[ttt.player]\n",
        "        action = player.get_action(ttt)\n",
        "\n",
        "        state, reward, done, player = ttt.step(action)\n",
        "\n",
        "    if ttt.winner is not None:\n",
        "        wins[ttt.winner] += 1\n",
        "\n",
        "print(f'X wins in {round((wins[\"X\"]/100_000)*100, 2)}% games')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88cH7Ym-Nb75"
      },
      "source": [
        "## –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å—Ä–∞–∑–∏—Ç—å—Å—è —Å –≤–∞—à–∏–º –æ–±—É—á–µ–Ω–Ω—ã–º –∞–≥–µ–Ω—Ç–æ–º\n",
        "\n",
        "–ú–æ–∂–µ—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å—Ä–∞–∑–∏—Ç—å—Å—è —Å –≤–∞—à–∏–º –æ–±—É—á–µ–Ω–Ω—ã–º –∞–≥–µ–Ω—Ç–æ–º.\n",
        "–í –∫–∞—á–µ—Å—Ç–≤–µ –≤–≤–æ–¥–∞ –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏–±–æ —Ä—É—á–Ω–æ–π –≤–≤–æ–¥, –ª–∏–±–æ —Å–ª—É—á–∞–π–Ω—ã–π:\n",
        "\n",
        "*   `human_action = int(input())  # manual input`\n",
        "*   `human_action = np.random.choice(ttt.legal_actions())`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdvyeuOjNb75"
      },
      "outputs": [],
      "source": [
        "ttt.reset()\n",
        "\n",
        "while not ttt.done:\n",
        "    ttt.render()\n",
        "    print(\"\\n\")\n",
        "    print(f\"It's {ttt.player} turn\")\n",
        "    if ttt.player == \"X\":\n",
        "        action = players[\"X\"].get_action(ttt)\n",
        "        state, reward, done, player = ttt.step(action)\n",
        "\n",
        "        ttt.render()\n",
        "        print(\"\\n\")\n",
        "        continue\n",
        "    else:\n",
        "        print(f\"chose action: {ttt.legal_actions()}\")\n",
        "        # human_action = int(input())  # manual input\n",
        "        human_action = np.random.choice(ttt.legal_actions())\n",
        "        state, reward, done, player = ttt.step(human_action)\n",
        "\n",
        "        ttt.render()\n",
        "        print(\"\\n\")\n",
        "\n",
        "print(f\"{ttt.winner} wins! Reward is {reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xtZZCDjNb76"
      },
      "source": [
        "# –ó–∞–¥–∞–Ω–∏–µ 3. –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –∏–≥—Ä–µ –≤ –∫—Ä–µ—Å—Ç–∏–∫–∏-–Ω–æ–ª–∏–∫–∏ –ø—Ä–∏ –ø–æ–º–æ—â–∏ DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D3MA6ltNb78"
      },
      "source": [
        "–û–±—É—á–∏—Ç–µ —Å –ø–æ–º–æ—â—å—é Deep Q-learning –∞–≥–µ–Ω—Ç–∞ –∏–≥—Ä–µ –≤ –∫—Ä–µ—Å—Ç–∏–∫–∏-–Ω–æ–ª–∏–∫–∏."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erSAqd8vNb79"
      },
      "source": [
        "## –§–æ—Ä–º–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
        "–û–±—É—á–µ–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é Deep Q-learning –∞–≥–µ–Ω—Ç, –∏–≥–∞—é—â–∏–π –∑–∞ `'X'`, —Å –≤–∏–Ω—Ä–µ–π—Ç–æ–º –ø—Ä–æ—Ç–∏–≤ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, –∏–≥—Ä–∞—é—â–µ–≥–æ –∑–∞ `'0'`, ~85%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEPjlrl-Nb79"
      },
      "source": [
        "## –ö–æ–¥ DQN –∞–≥–µ–Ω—Ç–∞"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "823U1iHyNb79"
      },
      "source": [
        "–î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è experience replay –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `deque` [‚úèÔ∏è[blog]](https://proproprogs.ru/structure_data/std-ochered-collectionsdeque-na-python)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j8eHDO0Nb7-"
      },
      "source": [
        "–í —ç—Ç–æ–π —á–∞—Å—Ç–∏ –∑–∞–¥–∞–Ω–∏—è –≤–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å DQN –∞–≥–µ–Ω—Ç–∞.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD94pA6LNb7-"
      },
      "source": [
        "–í–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–∏—Ç—å `# Your code here`:\n",
        "- –º–µ—Ç–æ–¥ `set_Q_network`: –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å–µ—Ç–∏. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —É—Å—Ç—Ä–æ–µ–Ω–∞ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º: –Ω–∞ –≤—Ö–æ–¥ –ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è 9 –∑–Ω–∞—á–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—é –∏–≥—Ä–æ–≤–æ–≥–æ –ø–æ–ª—è, –Ω–∞ –≤—ã—Ö–æ–¥ –≤—ã–¥–∞—é—Ç—Å—è 9 –∑–Ω–∞—á–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç Q-–∑–Ω–∞—á–µ–Ω–∏—è–º –¥–ª—è 9 –≤–æ–∑–º–æ–∂–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π, —Å–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏ ‚Äî –Ω–∞ –≤–∞—à –≤—ã–±–æ—Ä, –ø–æ—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å —Ä–∞–∑–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏.\n",
        "- –º–µ—Ç–æ–¥ `get_action`: –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –∏–ª–∏ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∞–≥–µ–Ω—Ç–∞ –∏ –∑–Ω–∞—á–µ–Ω–∏—è `epsilon`. –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `env.legal_actions()`.\n",
        "- –º–µ—Ç–æ–¥ `update_target_network`: –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∏ `self.Q_net`(–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è) –≤ —Å–µ—Ç—å `self.target_net` (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Q-–∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è —Å–ª–µ–¥—É—é—â–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è $s'$). –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `state_dict()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7vpZKmeNb7-"
      },
      "source": [
        "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/Exercises/EX15/dqn_loss.png\" alt=\"Drawing\" width=\"800\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLrBxRRZNb7_"
      },
      "source": [
        "**–°–æ–≤–µ—Ç:** –æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –Ω–µ –≤—Å–µ 9 –∑–Ω–∞—á–µ–Ω–∏–π –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π, —É—á—Ç–∏—Ç–µ —ç—Ç–æ –≤ –º–µ—Ç–æ–¥–µ `get_action`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU-ZltfmNb7_"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "\n",
        "class DQNAgent(nn.Module):\n",
        "    def __init__(\n",
        "        self, gamma=0.9, mark=\"X\", memory_size=10000, epsilon=0.,\n",
        "        epsilon_off=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        alpha: float\n",
        "            learning rate\n",
        "        gamma: float\n",
        "            discount coefficient\n",
        "        mark: str\n",
        "            'X' or '0' - player symbol\n",
        "        memory_size: int\n",
        "            size of memory buffer\n",
        "        epsilon: float\n",
        "            epsilon for epsilon-greedy agent\n",
        "        epsilon_off: boole\n",
        "            if True -'greedy' learning strategy or inference,\n",
        "            if False 'epsilon-greedy' learning strategy\n",
        "        \"\"\"\n",
        "        super(DQNAgent, self).__init__()\n",
        "        self.mark = mark\n",
        "        self.gamma = torch.tensor(gamma, dtype=float)\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_off = epsilon_off\n",
        "\n",
        "        # Experience replay\n",
        "        self.exp_replay = deque(maxlen=memory_size)\n",
        "        # Q-Network (for learning and Q(s, a))\n",
        "        self.Q_net = self.set_Q_network()\n",
        "        # Target-Network (for Q(s', a'))\n",
        "        self.target_net = self.set_Q_network()\n",
        "        self.update_target_network()\n",
        "\n",
        "    def set_Q_network(self):\n",
        "        \"\"\"Set Q_net architecture.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Q_net: nn.Sequential\n",
        "            Q_net architecture\n",
        "        \"\"\"\n",
        "        Q_net = nn.Sequential( # Your code here\n",
        "        return Q_net\n",
        "\n",
        "    def forward(self, states):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        states: list, np.array or torch.Tensor [batch_size, 9]\n",
        "            batch of environment states at the beginning of the agent's action\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Q_vals: torch.Tensor [batch_size, 9]\n",
        "            Q-vals for all 9 action (not all of this action legal)\n",
        "        \"\"\"\n",
        "        states = torch.Tensor(states)\n",
        "        Q_vals = self.Q_net(states)\n",
        "        return Q_vals\n",
        "\n",
        "    def get_action(self, Q_vals, env):\n",
        "        \"\"\"\n",
        "        Sample optimal or random legal action.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Q_vals: torch.Tensor [batch_size, 9]\n",
        "            Q-vals for all 9 action (not all of this action legal)\n",
        "        env: TicTacToeEnv\n",
        "            environment\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        action: int\n",
        "            number of cell for change\n",
        "        \"\"\"\n",
        "        state = torch.Tensor(env.cells)\n",
        "        # Get legal action from env, transform to torch.int64 tensor\n",
        "\n",
        "        legal_actions = # Your code here\n",
        "\n",
        "        index = torch.zeros(9, dtype=bool)\n",
        "        index[legal_actions.to(torch.int64)] = True\n",
        "        rand = random.uniform(0, 1)\n",
        "        if self.epsilon_off or rand >= self.epsilon:\n",
        "            # Sample optimal action\n",
        "            if self.mark == \"X\":\n",
        "                best_q = # Your code here\n",
        "            else:\n",
        "                best_q = # Your code here\n",
        "            action = torch.logical_and(Q_vals == best_q, index).nonzero()[0].item()\n",
        "\n",
        "        else:\n",
        "            # Sample random action\n",
        "            action =  # Your code here\n",
        "\n",
        "        return int(action)\n",
        "\n",
        "    def add_to_memory(self, current_state, action, next_state, reward, done):\n",
        "        \"\"\"Add data to experience replay.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        current_state: torch.Tensor [batch_size, 9]\n",
        "            Current environment state\n",
        "        action: int\n",
        "            Current agent action\n",
        "        next_state: torch.Tensor [batch_size, 9]\n",
        "            Environment state after agent action and opponent action\n",
        "        reward: int\n",
        "            Reward\n",
        "        done: bool\n",
        "            Game over flag\n",
        "        \"\"\"\n",
        "        self.exp_replay.append((current_state, action, next_state, reward, done))\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Use Q_net parameters to update target_net.\"\"\"\n",
        "        # Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVW43RBUNb8A"
      },
      "source": [
        "## –ö–æ–¥ –æ–±—É—á–µ–Ω–∏—è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0Xwhx_hNb8A"
      },
      "source": [
        "–†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é, —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—â—É—é TD Loss:\n",
        "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_\\text{reference}(s,a) ] ^2 $$\n",
        "\n",
        "–° Q-reference, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –∫–∞–∫:\n",
        "\n",
        "$$ \\large Q_\\text{reference}(s,a) = R^a_{s} + \\gamma \\cdot \\max_{a'} Q_\\text{target}(s', a'), $$\n",
        "\n",
        "–≥–¥–µ:\n",
        "* $R^a_{s}$ ‚Äî –Ω–∞–≥—Ä–∞–¥–∞ `reward` –∑–∞ –¥–µ–π—Å—Ç–≤–∏–µ `a` –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è `s`,\n",
        "* $Q_\\text{target}(s',a')$ ‚Äî $Q$-–∑–Ω–∞—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã `next_states` –∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è, –≤—ã—á–∏—Å–ª–µ–Ω–Ω–æ–µ `agent.target_net`,\n",
        "* $s, a, s'$ ‚Äî —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ `states`, –¥–µ–π—Å—Ç–≤–∏–µ `actions` –∏ —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ `next_states`,\n",
        "* $\\gamma$ ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è `agent.gamma`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqTNHB63Nb8B"
      },
      "source": [
        "**–°–æ–≤–µ—Ç:**\n",
        "- –ü—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ $Q_\\text{reference}(s,a)$ `reference_q` —É—á—Ç–∏—Ç–µ, —á—Ç–æ —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–≥—Ä—ã –Ω–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ Q-—Ç–∞–±–ª–∏—Ü–µ (–∏–∑ –Ω–µ–≥–æ —É–∂–µ –Ω–µ–ª—å–∑—è –¥–µ–ª–∞—Ç—å —Ö–æ–¥) –∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏–π, –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–∏—Ö –µ–º—É, $\\max_{a'} Q_\\text{target}(s', a')$ –±—É–¥–µ—Ç —Ä–∞–≤–Ω–æ –Ω—É–ª—é (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `is_not_done`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di3FYVVYNb8B"
      },
      "outputs": [],
      "source": [
        "def compute_td_loss(batch, agent):\n",
        "    states = torch.tensor(np.array([x[0] for x in batch]), dtype=torch.float)\n",
        "    actions = torch.tensor(np.array([x[1] for x in batch]), dtype=torch.int64)\n",
        "    next_states = torch.tensor(np.array([x[2] for x in batch]), dtype=torch.float)\n",
        "    rewards = torch.tensor(np.array([x[3] for x in batch]), dtype=torch.int64)\n",
        "    is_not_done = 1 - torch.tensor(np.array([x[4] for x in batch]), dtype=torch.int64)\n",
        "\n",
        "    q_vals = # Your code here\n",
        "    current_q = # Your code here\n",
        "    target_q = # Your code here\n",
        "    reference_q = # Your code here\n",
        "    loss = # Your code here\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWmcsTfSNb8B"
      },
      "source": [
        "–†–µ–∞–ª–∏–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –≤—ã–¥–∞—é—â—É—é –≤–∏–Ω—Ä–µ–π—Ç –∏–≥—Ä–æ–∫–∞ –Ω–∞ 10–∫ –∏–≥—Ä–∞—Ö (–Ω–µ –∑–∞–±—ã–≤–∞–µ–º –≤—ã–∫–ª—é—á–∞—Ç—å –ø—Ä–∏–º–µ—à–∏–≤–∞–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQTn6V4_Nb8B"
      },
      "outputs": [],
      "source": [
        "def get_winrate():\n",
        "    wins = {\"X\": 0, \"0\": 0}\n",
        "    players[\"X\"].epsilon_off = True\n",
        "\n",
        "    for i in range(10_000):\n",
        "        ttt.reset()\n",
        "\n",
        "        while not ttt.done:\n",
        "            player = players[ttt.player]\n",
        "            if player.mark == \"0\":\n",
        "                action = player.get_action(ttt)\n",
        "            else:\n",
        "                q_vals = player(ttt.cells)\n",
        "                action = player.get_action(q_vals, ttt)\n",
        "\n",
        "            state, reward, done, player = ttt.step(action)\n",
        "\n",
        "        if ttt.winner is not None:\n",
        "            wins[ttt.winner] += 1\n",
        "    wr = (wins[\"X\"] / 10000) * 100\n",
        "    print(f\"X wins in {round(wr, 2)}% games\")\n",
        "    return wr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyYHr5IjNb8C"
      },
      "outputs": [],
      "source": [
        "target_wr = 85.0\n",
        "\n",
        "ttt = TicTacToeEnv()\n",
        "\n",
        "x_agent = DQNAgent(gamma=0.8, mark=\"X\", epsilon=0.2, epsilon_off=False)\n",
        "o_agent = RandomAgent(\"0\")\n",
        "\n",
        "players = {\"X\": x_agent, \"0\": o_agent}\n",
        "\n",
        "opt = torch.optim.Adam(players[\"X\"].Q_net.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St4cANmRNb8C"
      },
      "source": [
        "–î–∞–ª–µ–µ –º—ã –æ–±—É—á–∏–º –∏–≥—Ä–æ–∫–∞ `X` –ø—Ä–æ—Ç–∏–≤ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∏–≥—Ä–æ–∫–∞ `0`. –ó–∞–¥–∞—á–∞ ‚Äî –¥–æ–±–∏—Ç—å—Å—è –≤–∏–Ω—Ä–µ–π—Ç–∞ ~85%.\n",
        "\n",
        "–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä–æ–∏—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n",
        "* –≤–æ –≤—Ä–µ–º—è –æ–¥–Ω–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ 50 –∏–≥—Ä —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è –∏–≥—Ä–æ–≤–æ–π –æ–ø—ã—Ç –≤ –±—É—Ñ–µ—Ä –ø–∞–º—è—Ç–∏ –∞–≥–µ–Ω—Ç–∞, –∞ –∏–º–µ–Ω–Ω–æ —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –¥–µ–π—Å—Ç–≤–∏–µ –∏–∑ –Ω–µ–≥–æ, —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –Ω–∞–≥—Ä–∞–¥–∞ –∏ —Ñ–ª–∞–≥ –∫–æ–Ω—Ü–∞ —ç–ø–∏–∑–æ–¥–∞;\n",
        "* –∏–∑ –ø–∞–º—è—Ç–∏ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Å–ª—É—á–∞–π–Ω—ã–π –±–∞—Ç—á –∏ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –≤–µ—Å–∞ Q-—Å–µ—Ç–∏;\n",
        "* –ø–æ—Å–ª–µ –∫–∞–∂–¥—ã—Ö 50 –ø–æ–¥–æ–±–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –≤–µ—Å–∞ target network, —Å–ª–µ–¥–∏—Ç–µ –∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –≤–∏–Ω—Ä–µ–π—Ç–∞ –ø–æ—Å–ª–µ –∫–∞–∂–¥—ã—Ö 500 —ç–ø–∏–∑–æ–¥–æ–≤.\n",
        "\n",
        "–í–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ–ø—É—Å–∫ `# Your code here`, –≤ –∫–æ—Ç–æ—Ä–æ–º –∑–∞–ø–æ–ª–Ω—è–µ—Ç—Å—è –±—É—Ñ–µ—Ä –ø–∞–º—è—Ç–∏ –∞–≥–µ–Ω—Ç–∞."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAf2eKnUNb8C"
      },
      "source": [
        "**–°–æ–≤–µ—Ç:**\n",
        "- –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —É—á—Ç–∏—Ç–µ, —á—Ç–æ $s'$ –±—É–¥–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –∏–≥—Ä–æ–≤–æ–≥–æ –ø–æ–ª—è –Ω–µ –ø–æ—Å–ª–µ —Ö–æ–¥–∞ –∏–≥—Ä–æ–∫–∞, –∞ –ø–æ—Å–ª–µ —Ö–æ–¥–∞ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞.\n",
        "- –û–±—É—á–µ–Ω–∏–µ RL-–º–æ–¥–µ–ª–∏ –Ω–µ—É—Å—Ç–æ–π—á–∏–≤–æ. –ù–µ–±–æ–ª—å—à–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å–µ—Ç–∏ –∏–ª–∏ –¥–∞–∂–µ [–≤—ã–±–æ—Ä seed ‚úèÔ∏è[blog]](https://www.alexirpan.com/2018/02/14/rl-hard.html) –º–æ–≥—É—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ–º–µ–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2aT2WlvNb8D"
      },
      "outputs": [],
      "source": [
        "clear_output()\n",
        "\n",
        "for j in range(10_000):\n",
        "    for i in range(50):\n",
        "        # Your code here\n",
        "\n",
        "    batch = random.sample(players[\"X\"].exp_replay, 128)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss = compute_td_loss(batch, players[\"X\"])\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(players[\"X\"].Q_net.parameters(), 1.0)\n",
        "    opt.step()\n",
        "\n",
        "    if j % 50 == 0:\n",
        "        players[\"X\"].update_target_network()\n",
        "\n",
        "    if j % 500 == 0:\n",
        "        wr = get_winrate()\n",
        "\n",
        "    if wr > target_wr:\n",
        "        break\n",
        "        if players[\"X\"].epsilon > 0:\n",
        "            players[\"X\"].epsilon -= 0.005"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}